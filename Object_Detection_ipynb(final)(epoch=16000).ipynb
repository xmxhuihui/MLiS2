{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Object Detection.ipynb(final)(epoch=15000)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xmxhuihui/MLiS2/blob/master/Object_Detection_ipynb(final)(epoch%3D16000).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7pZgQ8itosn",
        "colab_type": "code",
        "outputId": "f6b8f464-1165-493f-9f6e-2f04bd758bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp8A2H9qxGBQ",
        "colab_type": "code",
        "outputId": "05d8038f-067c-452f-da58-fdf260b3999f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVoclS7Ptosu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training steps.\n",
        "num_steps = 1000  # 200000\n",
        "#num_steps = 100  # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "# model configs are from Model Zoo github: \n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "MODELS_CONFIG = {\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
        "    'ssd_mobilenet_v1_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
        "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'batch_size': 12\n",
        "    },    \n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "    'ssd_mobilenet_v2_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 12\n",
        "    }\n",
        "}\n",
        "selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCsS9rrttosy",
        "colab_type": "code",
        "outputId": "29db8acb-3b44-48aa-a258-c30639890e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "!pip install -U -q numpy==1.17.0\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\u001b[K     |████████████████████████████████| 20.4MB 1.7MB/s \n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h/content/models/research\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_experimental_model\n",
            "[       OK ] ModelBuilderTest.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 17 tests in 0.241s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CY8NF_Mtos4",
        "colab_type": "code",
        "outputId": "60e7b0a2-cd57-4d72-8e32-3c0fa3afa576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "%cd /content/drive/My Drive/Colab Notebooks\n",
        "!python code/xml_to_csv.py -i Object -o annotations/train_labels.csv -l annotations\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i Object -o annotations/test_labels.csv\n",
        "\n",
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=annotations/train_labels.csv --output_path=annotations/train.record --img_path=Object --label_map annotations/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=annotations/test_labels.csv --output_path=annotations/test.record --img_path=Object --label_map annotations/label_map.pbtxt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n",
            "Successfully converted xml to csv.\n",
            "Generate `annotations/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n",
            "WARNING:tensorflow:From code/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0503 16:18:01.943094 140070056109952 module_wrapper.py:139] From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0503 16:18:02.609474 140070056109952 module_wrapper.py:139] From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/drive/My Drive/Colab Notebooks/annotations/train.record\n",
            "WARNING:tensorflow:From code/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0503 16:19:03.208383 140397043120000 module_wrapper.py:139] From code/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0503 16:19:03.944317 140397043120000 module_wrapper.py:139] From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/drive/My Drive/Colab Notebooks/annotations/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtDeNgOotos7",
        "colab_type": "code",
        "outputId": "28c4b5fa-a321-4885-aef2-5e8f07775999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_record_fname = '/content/drive/My Drive/Colab Notebooks/annotations/test.record'\n",
        "train_record_fname = '/content/drive/My Drive/Colab Notebooks/annotations/train.record'\n",
        "label_map_pbtxt_fname = '/content/drive/My Drive/Colab Notebooks/annotations/label_map.pbtxt'\n",
        "print(test_record_fname)\n",
        "print(train_record_fname)\n",
        "print(label_map_pbtxt_fname)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/annotations/test.record\n",
            "/content/drive/My Drive/Colab Notebooks/annotations/train.record\n",
            "/content/drive/My Drive/Colab Notebooks/annotations/label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8heNtyztos-",
        "colab_type": "code",
        "outputId": "69abdfe5-55dd-4bed-c4b4-922913e9a126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat annotations/train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filename,width,height,class,xmin,ymin,xmax,ymax\n",
            "1582722545700_95_0.png,320,240,large female,179,68,238,223\n",
            "1584031891105_90_35.png,320,240,small male,108,37,125,70\n",
            "1584031891105_90_35.png,320,240,red light,122,10,130,32\n",
            "1584031891105_90_35.png,320,240,left sign,135,16,143,32\n",
            "1584029456776_90_0.png,320,240,small male,160,69,187,139\n",
            "1584031273877_90_35.png,320,240,green light,129,10,138,42\n",
            "1584031273877_90_35.png,320,240,left sign,147,17,160,44\n",
            "1584029455744_90_0.png,320,240,small male,161,69,187,140\n",
            "1582723269156_85_0.png,320,240,large male,111,62,164,236\n",
            "1582723269156_85_0.png,320,240,large female,63,61,109,217\n",
            "1584031274909_90_35.png,320,240,green light,126,13,139,51\n",
            "1584031274909_90_35.png,320,240,left sign,146,21,161,53\n",
            "1584031896275_90_0.png,320,240,small male,61,85,100,182\n",
            "1584031896275_90_0.png,320,240,red light,122,10,132,34\n",
            "1584031896275_90_0.png,320,240,left sign,138,17,149,35\n",
            "1584031896275_90_0.png,320,240,small female,55,12,65,38\n",
            "1582727151921_95_35.png,320,240,large male,126,55,150,115\n",
            "1582727151921_95_35.png,320,240,large female,166,57,184,109\n",
            "1584029457812_90_0.png,320,240,small male,160,70,187,140\n",
            "1584031892138_90_35.png,320,240,small male,102,42,124,84\n",
            "1584031892138_90_35.png,320,240,red light,123,11,133,33\n",
            "1584031892138_90_35.png,320,240,left sign,136,16,147,33\n",
            "1582722551904_105_0.png,320,240,large female,207,71,274,240\n",
            "1582727202252_105_35.png,320,240,tree,203,73,255,147\n",
            "1582727202252_105_35.png,320,240,box,248,91,299,140\n",
            "1582723269156_85_0 2.png,320,240,large male,112,63,163,236\n",
            "1582723269156_85_0 2.png,320,240,large female,63,61,109,216\n",
            "1584031448675_90_35.png,320,240,red light,137,15,160,76\n",
            "1584031448675_90_35.png,320,240,right sign,179,31,209,83\n",
            "1584031450743_90_0.png,320,240,red light,136,16,160,84\n",
            "1584031450743_90_0.png,320,240,right sign,182,35,216,90\n",
            "1584031446607_100_35.png,320,240,red light,164,15,180,59\n",
            "1584031446607_100_35.png,320,240,right sign,193,28,214,63\n",
            "1584031447642_90_35.png,320,240,red light,142,15,163,65\n",
            "1584031447642_90_35.png,320,240,right sign,177,29,203,70\n",
            "1584031113657_90_35.png,320,240,left sign,118,25,146,78\n",
            "1584031113657_90_35.png,320,240,green light,167,20,189,83\n",
            "1584031911818_80_0.png,320,240,green light,181,21,208,98\n",
            "1584031911818_80_0.png,320,240,left sign,234,42,262,105\n",
            "1584031275945_85_35.png,320,240,green light,135,13,150,56\n",
            "1584031275945_85_35.png,320,240,left sign,159,23,176,60\n",
            "1584031161017_90_35.png,320,240,green light,122,15,147,83\n",
            "1584031161017_90_35.png,320,240,right sign,164,32,199,88\n",
            "1584031906649_90_35.png,320,240,small female,15,15,33,79\n",
            "1584031906649_90_35.png,320,240,red light,164,18,179,62\n",
            "1584031906649_90_35.png,320,240,left sign,195,28,213,65\n",
            "1584031158949_90_35.png,320,240,green light,115,14,133,58\n",
            "1584031158949_90_35.png,320,240,right sign,144,23,166,60\n",
            "1584031912855_80_35.png,320,240,green light,199,24,224,107\n",
            "1584031912855_80_35.png,320,240,left sign,252,46,282,114\n",
            "1584031908718_75_35.png,320,240,red light,151,17,174,84\n",
            "1584031908718_75_35.png,320,240,left sign,196,33,222,87\n",
            "1584031278013_85_35.png,320,240,green light,149,17,171,82\n",
            "1584031278013_85_35.png,320,240,left sign,182,32,210,85\n",
            "1584031159985_85_35.png,320,240,green light,110,15,130,68\n",
            "1584031159985_85_35.png,320,240,right sign,144,25,171,70\n",
            "1584030496762_120_0.png,320,240,small male,175,100,228,197\n",
            "1584030496762_120_0.png,320,240,small female,232,96,277,180\n",
            "1584031451779_90_0.png,320,240,red light,135,17,160,85\n",
            "1584031451779_90_0.png,320,240,right sign,181,35,216,91\n",
            "1584031894206_90_35.png,320,240,small male,72,72,108,153\n",
            "1584031894206_90_35.png,320,240,red light,122,12,132,35\n",
            "1584031894206_90_35.png,320,240,left sign,138,18,148,36\n",
            "1584031456012_115_35.png,320,240,right sign,68,36,126,141\n",
            "1584031456012_115_35.png,320,240,green light,1,17,28,140\n",
            "1582727155026_105_35.png,320,240,large male,23,55,80,235\n",
            "1582727155026_105_35.png,320,240,large female,134,55,175,178\n",
            "1584031449711_90_0.png,320,240,red light,135,16,159,84\n",
            "1584031449711_90_0.png,320,240,right sign,182,34,216,91\n",
            "1582722575752_115_0.png,320,240,large male,175,71,246,240\n",
            "1584029576443_80_0.png,320,240,small female,276,82,307,131\n",
            "1582722564346_90_0.png,320,240,large female,153,63,205,224\n",
            "1582727152957_90_35.png,320,240,large male,127,55,152,133\n",
            "1582727152957_90_35.png,320,240,large female,177,58,198,121\n",
            "1584031445574_90_35.png,320,240,red light,163,16,178,54\n",
            "1584031445574_90_35.png,320,240,right sign,189,26,207,56\n",
            "1584029454708_90_0.png,320,240,small male,160,69,188,140\n",
            "1584031454952_110_35.png,320,240,green light,132,17,161,102\n",
            "1584031454952_110_35.png,320,240,right sign,191,41,234,110\n",
            "1582727204356_115_35.png,320,240,tree,1,83,85,240\n",
            "1582727204356_115_35.png,320,240,box,77,102,177,200\n",
            "1584031905617_95_35.png,320,240,small female,46,12,63,65\n",
            "1584031905617_95_35.png,320,240,red light,172,17,187,55\n",
            "1584031905617_95_35.png,320,240,left sign,197,28,216,58\n",
            "1582727201184_105_35.png,320,240,tree,264,79,302,137\n",
            "1584031114689_75_35.png,320,240,left sign,144,31,180,101\n",
            "1584031114689_75_35.png,320,240,green light,207,26,234,109\n",
            "1582723268088_85_0 2.png,320,240,large male,112,63,167,236\n",
            "1582727160263_105_35.png,320,240,tree,99,72,172,175\n",
            "1582722544668_95_0.png,320,240,large female,179,67,237,225\n",
            "1584031910786_80_0.png,320,240,red light,181,23,209,98\n",
            "1584031910786_80_0.png,320,240,left sign,233,41,261,104\n",
            "1582722761785_95_0.png,320,240,large male,128,61,167,174\n",
            "1582722761785_95_0.png,320,240,large female,164,64,207,194\n",
            "1584031112453_85_35.png,320,240,left sign,105,22,125,63\n",
            "1584031112453_85_35.png,320,240,green light,144,17,161,67\n",
            "1582727153990_100_35.png,320,240,large male,125,56,159,166\n",
            "1582727153990_100_35.png,320,240,large female,192,60,220,143\n",
            "1584030424721_115_0.png,320,240,large female,224,30,278,177\n",
            "1584031157917_95_35.png,320,240,green light,135,15,150,53\n",
            "1584031157917_95_35.png,320,240,right sign,159,24,181,56\n",
            "1582723270192_85_0.png,320,240,large male,110,63,164,237\n",
            "1582723270192_85_0.png,320,240,large female,62,62,110,216\n",
            "1584031444539_90_35.png,320,240,red light,147,13,161,45\n",
            "1584031444539_90_35.png,320,240,right sign,171,22,186,48\n",
            "1584031279045_75_35.png,320,240,green light,187,22,213,106\n",
            "1584031279045_75_35.png,320,240,left sign,228,42,259,112\n",
            "1582723268088_85_0.png,320,240,large male,111,62,164,236\n",
            "1584032031545_90_0.png,320,240,box,81,55,168,150\n",
            "1584032031545_90_0.png,320,240,right sign,111,20,132,53\n",
            "1584032031545_90_0.png,320,240,red light,144,15,157,54\n",
            "1584031895242_90_0.png,320,240,small male,61,86,100,183\n",
            "1584031895242_90_0.png,320,240,red light,123,11,132,35\n",
            "1584031895242_90_0.png,320,240,left sign,138,16,149,35\n",
            "1584031895242_90_0.png,320,240,small female,55,12,65,38\n",
            "1582723271224_85_0 2.png,320,240,large male,110,63,164,237\n",
            "1582723271224_85_0 2.png,320,240,large female,63,61,108,216\n",
            "1584029458844_90_0.png,320,240,small male,160,69,189,141\n",
            "1582727158158_100_35.png,320,240,tree,260,79,292,134\n",
            "1584031276978_90_35.png,320,240,green light,149,17,169,69\n",
            "1584031276978_90_35.png,320,240,left sign,178,27,201,72\n",
            "1584031453879_90_0.png,320,240,green light,135,16,162,84\n",
            "1584031453879_90_0.png,320,240,right sign,182,35,217,91\n",
            "1582727159195_115_35.png,320,240,tree,213,74,262,145\n",
            "1584031907681_95_35.png,320,240,red light,159,18,176,70\n",
            "1584031907681_95_35.png,320,240,left sign,194,31,216,74\n",
            "1584031893174_90_35.png,320,240,small male,92,51,118,107\n",
            "1584031893174_90_35.png,320,240,red light,123,10,132,34\n",
            "1584031893174_90_35.png,320,240,left sign,136,16,149,34\n",
            "1584029142663_110_0.png,320,240,tree,65,23,139,145\n",
            "1582723270192_85_0 2.png,320,240,large male,112,63,164,236\n",
            "1582723270192_85_0 2.png,320,240,large female,63,61,109,216\n",
            "1582722550872_105_0.png,320,240,large female,205,65,273,240\n",
            "1582727203288_110_35.png,320,240,tree,122,74,194,180\n",
            "1582727203288_110_35.png,320,240,box,187,94,255,159\n",
            "1584031909750_80_0.png,320,240,red light,181,22,210,98\n",
            "1584031909750_80_0.png,320,240,left sign,232,41,261,105\n",
            "1582727156058_120_35.png,320,240,large female,8,67,70,240\n",
            "1584029346081_95_0.png,320,240,small female,191,82,229,162\n",
            "1582722569547_105_0.png,320,240,large male,160,65,219,225\n",
            "1584031162053_115_35.png,320,240,green light,61,14,94,107\n",
            "1584031162053_115_35.png,320,240,right sign,117,32,163,108\n",
            "1584031452811_90_0.png,320,240,red light,134,16,164,85\n",
            "1584031452811_90_0.png,320,240,right sign,182,34,217,91\n",
            "1584029459880_90_0.png,320,240,small male,160,69,188,140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-2pfmMstotC",
        "colab_type": "code",
        "outputId": "613230c8-7c22-4733-8330-77da913e9f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat annotations/test_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filename,width,height,class,xmin,ymin,xmax,ymax\n",
            "1582722545700_95_0.png,320,240,large female,179,68,238,223\n",
            "1584031891105_90_35.png,320,240,small male,108,37,125,70\n",
            "1584031891105_90_35.png,320,240,red light,122,10,130,32\n",
            "1584031891105_90_35.png,320,240,left sign,135,16,143,32\n",
            "1584029456776_90_0.png,320,240,small male,160,69,187,139\n",
            "1584031273877_90_35.png,320,240,green light,129,10,138,42\n",
            "1584031273877_90_35.png,320,240,left sign,147,17,160,44\n",
            "1584029455744_90_0.png,320,240,small male,161,69,187,140\n",
            "1582723269156_85_0.png,320,240,large male,111,62,164,236\n",
            "1582723269156_85_0.png,320,240,large female,63,61,109,217\n",
            "1584031274909_90_35.png,320,240,green light,126,13,139,51\n",
            "1584031274909_90_35.png,320,240,left sign,146,21,161,53\n",
            "1584031896275_90_0.png,320,240,small male,61,85,100,182\n",
            "1584031896275_90_0.png,320,240,red light,122,10,132,34\n",
            "1584031896275_90_0.png,320,240,left sign,138,17,149,35\n",
            "1584031896275_90_0.png,320,240,small female,55,12,65,38\n",
            "1582727151921_95_35.png,320,240,large male,126,55,150,115\n",
            "1582727151921_95_35.png,320,240,large female,166,57,184,109\n",
            "1584029457812_90_0.png,320,240,small male,160,70,187,140\n",
            "1584031892138_90_35.png,320,240,small male,102,42,124,84\n",
            "1584031892138_90_35.png,320,240,red light,123,11,133,33\n",
            "1584031892138_90_35.png,320,240,left sign,136,16,147,33\n",
            "1582722551904_105_0.png,320,240,large female,207,71,274,240\n",
            "1582727202252_105_35.png,320,240,tree,203,73,255,147\n",
            "1582727202252_105_35.png,320,240,box,248,91,299,140\n",
            "1582723269156_85_0 2.png,320,240,large male,112,63,163,236\n",
            "1582723269156_85_0 2.png,320,240,large female,63,61,109,216\n",
            "1584031448675_90_35.png,320,240,red light,137,15,160,76\n",
            "1584031448675_90_35.png,320,240,right sign,179,31,209,83\n",
            "1584031450743_90_0.png,320,240,red light,136,16,160,84\n",
            "1584031450743_90_0.png,320,240,right sign,182,35,216,90\n",
            "1584031446607_100_35.png,320,240,red light,164,15,180,59\n",
            "1584031446607_100_35.png,320,240,right sign,193,28,214,63\n",
            "1584031447642_90_35.png,320,240,red light,142,15,163,65\n",
            "1584031447642_90_35.png,320,240,right sign,177,29,203,70\n",
            "1584031113657_90_35.png,320,240,left sign,118,25,146,78\n",
            "1584031113657_90_35.png,320,240,green light,167,20,189,83\n",
            "1584031911818_80_0.png,320,240,green light,181,21,208,98\n",
            "1584031911818_80_0.png,320,240,left sign,234,42,262,105\n",
            "1584031275945_85_35.png,320,240,green light,135,13,150,56\n",
            "1584031275945_85_35.png,320,240,left sign,159,23,176,60\n",
            "1584031161017_90_35.png,320,240,green light,122,15,147,83\n",
            "1584031161017_90_35.png,320,240,right sign,164,32,199,88\n",
            "1584031906649_90_35.png,320,240,small female,15,15,33,79\n",
            "1584031906649_90_35.png,320,240,red light,164,18,179,62\n",
            "1584031906649_90_35.png,320,240,left sign,195,28,213,65\n",
            "1584031158949_90_35.png,320,240,green light,115,14,133,58\n",
            "1584031158949_90_35.png,320,240,right sign,144,23,166,60\n",
            "1584031912855_80_35.png,320,240,green light,199,24,224,107\n",
            "1584031912855_80_35.png,320,240,left sign,252,46,282,114\n",
            "1584031908718_75_35.png,320,240,red light,151,17,174,84\n",
            "1584031908718_75_35.png,320,240,left sign,196,33,222,87\n",
            "1584031278013_85_35.png,320,240,green light,149,17,171,82\n",
            "1584031278013_85_35.png,320,240,left sign,182,32,210,85\n",
            "1584031159985_85_35.png,320,240,green light,110,15,130,68\n",
            "1584031159985_85_35.png,320,240,right sign,144,25,171,70\n",
            "1584030496762_120_0.png,320,240,small male,175,100,228,197\n",
            "1584030496762_120_0.png,320,240,small female,232,96,277,180\n",
            "1584031451779_90_0.png,320,240,red light,135,17,160,85\n",
            "1584031451779_90_0.png,320,240,right sign,181,35,216,91\n",
            "1584031894206_90_35.png,320,240,small male,72,72,108,153\n",
            "1584031894206_90_35.png,320,240,red light,122,12,132,35\n",
            "1584031894206_90_35.png,320,240,left sign,138,18,148,36\n",
            "1584031456012_115_35.png,320,240,right sign,68,36,126,141\n",
            "1584031456012_115_35.png,320,240,green light,1,17,28,140\n",
            "1582727155026_105_35.png,320,240,large male,23,55,80,235\n",
            "1582727155026_105_35.png,320,240,large female,134,55,175,178\n",
            "1584031449711_90_0.png,320,240,red light,135,16,159,84\n",
            "1584031449711_90_0.png,320,240,right sign,182,34,216,91\n",
            "1582722575752_115_0.png,320,240,large male,175,71,246,240\n",
            "1584029576443_80_0.png,320,240,small female,276,82,307,131\n",
            "1582722564346_90_0.png,320,240,large female,153,63,205,224\n",
            "1582727152957_90_35.png,320,240,large male,127,55,152,133\n",
            "1582727152957_90_35.png,320,240,large female,177,58,198,121\n",
            "1584031445574_90_35.png,320,240,red light,163,16,178,54\n",
            "1584031445574_90_35.png,320,240,right sign,189,26,207,56\n",
            "1584029454708_90_0.png,320,240,small male,160,69,188,140\n",
            "1584031454952_110_35.png,320,240,green light,132,17,161,102\n",
            "1584031454952_110_35.png,320,240,right sign,191,41,234,110\n",
            "1582727204356_115_35.png,320,240,tree,1,83,85,240\n",
            "1582727204356_115_35.png,320,240,box,77,102,177,200\n",
            "1584031905617_95_35.png,320,240,small female,46,12,63,65\n",
            "1584031905617_95_35.png,320,240,red light,172,17,187,55\n",
            "1584031905617_95_35.png,320,240,left sign,197,28,216,58\n",
            "1582727201184_105_35.png,320,240,tree,264,79,302,137\n",
            "1584031114689_75_35.png,320,240,left sign,144,31,180,101\n",
            "1584031114689_75_35.png,320,240,green light,207,26,234,109\n",
            "1582723268088_85_0 2.png,320,240,large male,112,63,167,236\n",
            "1582727160263_105_35.png,320,240,tree,99,72,172,175\n",
            "1582722544668_95_0.png,320,240,large female,179,67,237,225\n",
            "1584031910786_80_0.png,320,240,red light,181,23,209,98\n",
            "1584031910786_80_0.png,320,240,left sign,233,41,261,104\n",
            "1582722761785_95_0.png,320,240,large male,128,61,167,174\n",
            "1582722761785_95_0.png,320,240,large female,164,64,207,194\n",
            "1584031112453_85_35.png,320,240,left sign,105,22,125,63\n",
            "1584031112453_85_35.png,320,240,green light,144,17,161,67\n",
            "1582727153990_100_35.png,320,240,large male,125,56,159,166\n",
            "1582727153990_100_35.png,320,240,large female,192,60,220,143\n",
            "1584030424721_115_0.png,320,240,large female,224,30,278,177\n",
            "1584031157917_95_35.png,320,240,green light,135,15,150,53\n",
            "1584031157917_95_35.png,320,240,right sign,159,24,181,56\n",
            "1582723270192_85_0.png,320,240,large male,110,63,164,237\n",
            "1582723270192_85_0.png,320,240,large female,62,62,110,216\n",
            "1584031444539_90_35.png,320,240,red light,147,13,161,45\n",
            "1584031444539_90_35.png,320,240,right sign,171,22,186,48\n",
            "1584031279045_75_35.png,320,240,green light,187,22,213,106\n",
            "1584031279045_75_35.png,320,240,left sign,228,42,259,112\n",
            "1582723268088_85_0.png,320,240,large male,111,62,164,236\n",
            "1584032031545_90_0.png,320,240,box,81,55,168,150\n",
            "1584032031545_90_0.png,320,240,right sign,111,20,132,53\n",
            "1584032031545_90_0.png,320,240,red light,144,15,157,54\n",
            "1584031895242_90_0.png,320,240,small male,61,86,100,183\n",
            "1584031895242_90_0.png,320,240,red light,123,11,132,35\n",
            "1584031895242_90_0.png,320,240,left sign,138,16,149,35\n",
            "1584031895242_90_0.png,320,240,small female,55,12,65,38\n",
            "1582723271224_85_0 2.png,320,240,large male,110,63,164,237\n",
            "1582723271224_85_0 2.png,320,240,large female,63,61,108,216\n",
            "1584029458844_90_0.png,320,240,small male,160,69,189,141\n",
            "1582727158158_100_35.png,320,240,tree,260,79,292,134\n",
            "1584031276978_90_35.png,320,240,green light,149,17,169,69\n",
            "1584031276978_90_35.png,320,240,left sign,178,27,201,72\n",
            "1584031453879_90_0.png,320,240,green light,135,16,162,84\n",
            "1584031453879_90_0.png,320,240,right sign,182,35,217,91\n",
            "1582727159195_115_35.png,320,240,tree,213,74,262,145\n",
            "1584031907681_95_35.png,320,240,red light,159,18,176,70\n",
            "1584031907681_95_35.png,320,240,left sign,194,31,216,74\n",
            "1584031893174_90_35.png,320,240,small male,92,51,118,107\n",
            "1584031893174_90_35.png,320,240,red light,123,10,132,34\n",
            "1584031893174_90_35.png,320,240,left sign,136,16,149,34\n",
            "1584029142663_110_0.png,320,240,tree,65,23,139,145\n",
            "1582723270192_85_0 2.png,320,240,large male,112,63,164,236\n",
            "1582723270192_85_0 2.png,320,240,large female,63,61,109,216\n",
            "1582722550872_105_0.png,320,240,large female,205,65,273,240\n",
            "1582727203288_110_35.png,320,240,tree,122,74,194,180\n",
            "1582727203288_110_35.png,320,240,box,187,94,255,159\n",
            "1584031909750_80_0.png,320,240,red light,181,22,210,98\n",
            "1584031909750_80_0.png,320,240,left sign,232,41,261,105\n",
            "1582727156058_120_35.png,320,240,large female,8,67,70,240\n",
            "1584029346081_95_0.png,320,240,small female,191,82,229,162\n",
            "1582722569547_105_0.png,320,240,large male,160,65,219,225\n",
            "1584031162053_115_35.png,320,240,green light,61,14,94,107\n",
            "1584031162053_115_35.png,320,240,right sign,117,32,163,108\n",
            "1584031452811_90_0.png,320,240,red light,134,16,164,85\n",
            "1584031452811_90_0.png,320,240,right sign,182,34,217,91\n",
            "1584029459880_90_0.png,320,240,small male,160,69,188,140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO6IJm-TtotE",
        "colab_type": "text"
      },
      "source": [
        "## Download Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwVVJ17ctotE",
        "colab_type": "code",
        "outputId": "d07f49bd-25c5-483b-c6a4-3b19eb2a7fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "%cd /content/models/research\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "    \n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBEOSKFrtotH",
        "colab_type": "code",
        "outputId": "905feb65-0063-4a1b-e30f-8317d2d5a8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/pretrained_model\n",
            "total 204M\n",
            "drwx------  2 303230 5000 4.0K Jan  4  2019 .\n",
            "drwxr-xr-x 63 root   root 4.0K May  3 16:19 ..\n",
            "-rw-------  1 303230 5000  93M Jan  4  2019 model.ckpt.data-00000-of-00001\n",
            "-rw-------  1 303230 5000  68K Jan  4  2019 model.ckpt.index\n",
            "-rw-------  1 303230 5000  20M Jan  4  2019 model.ckpt.meta\n",
            "-rw-------  1 303230 5000 4.3K Jan  4  2019 pipeline.config\n",
            "-rw-------  1 303230 5000  24M Jan  4  2019 tflite_graph.pb\n",
            "-rw-------  1 303230 5000  68M Jan  4  2019 tflite_graph.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piFYGXcbtotI",
        "colab_type": "code",
        "outputId": "65586ee0-41c1-4730-91b3-965adc9d4266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yfEXnVtotK",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMShEscXtotK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmHSwMtDtotM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnpkEGmItotO",
        "colab_type": "code",
        "outputId": "d33fe942-5808-4dcb-cc0e-ab0f1773a81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import re  #regular expression\n",
        "\n",
        "# training pipeline file defines:\n",
        "# - pretrain model path\n",
        "# - the train/test sets\n",
        "# - ID to Label mapping and number of classes\n",
        "# - training batch size\n",
        "# - epochs to trains\n",
        "# - learning rate\n",
        "# - etc\n",
        "\n",
        "# note we just need to use a sample one, and make edits to it.\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test, we created earlier with our training/test sets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path: ID to label file\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps (Number of epochs to train)\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/label_map_util.py:138: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQhXOhhptotQ",
        "colab_type": "code",
        "outputId": "4396b6f1-8a67-48a4-cd40-023f93d2046d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "!cat \"{label_map_pbtxt_fname}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "    id: 1\n",
            "    name: 'box'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 2\n",
            "    name: 'green light'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 3\n",
            "    name: 'large female'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 4\n",
            "    name: 'large male'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 5\n",
            "    name: 'left sign'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 6\n",
            "    name: 'red light'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 7\n",
            "    name: 'right sign'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 8\n",
            "    name: 'small female'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 9\n",
            "    name: 'small male'\n",
            "}\n",
            "\n",
            "item {\n",
            "    id: 10\n",
            "    name: 'tree'\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZnXCG46totR",
        "colab_type": "code",
        "outputId": "7e1d96b7-c65a-45c6-c125-44370ff9ec3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat {pipeline_fname}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Quantized trained SSD with Mobilenet v2 on MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 10\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 12\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 1000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/drive/My Drive/Colab Notebooks/annotations/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/drive/My Drive/Colab Notebooks/annotations/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/drive/My Drive/Colab Notebooks/annotations/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/drive/My Drive/Colab Notebooks/annotations/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}\n",
            "\n",
            "graph_rewriter {\n",
            "  quantization {\n",
            "    delay: 48000\n",
            "    weight_bits: 8\n",
            "    activation_bits: 8\n",
            "  }\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKkgjtwURhe",
        "colab_type": "code",
        "outputId": "249d228a-b95e-480a-e81c-796e39c27b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip\n",
        "model_dir = '/content/drive/My Drive/Colab Notebooks/Training'\n",
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-03 16:21:54--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.226.171.201, 3.221.253.104, 3.228.72.85, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.226.171.201|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  6.49MB/s    in 2.0s    \n",
            "\n",
            "2020-05-03 16:21:57 (6.49 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://1e8c0396.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXkh7vbjtotS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = 16000\n",
        "\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir='{model_dir}' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dEmwmQgtotT",
        "colab_type": "text"
      },
      "source": [
        "# Save and Convert Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTZmQneKtotT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '%s/fine_tuned_model' % model_dir\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTORp89gtotV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lst = os.listdir(model_dir)\n",
        "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRcINE-StotW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo creates the frozen inference graph in fine_tune_model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jAZM0K0totX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
        "# create the tensorflow lite graph\n",
        "#!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "#    --pipeline_config_path={pipeline_fname} \\\n",
        "#    --trained_checkpoint_prefix='{last_model_path}' \\\n",
        "#    --output_directory='{output_directory}' \\\n",
        "#    --add_postprocessing_op=true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_r9x_HptotY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
        "#!tflite_convert \\\n",
        "#  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n",
        "#  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "#  --inference_type=QUANTIZED_UINT8 \\\n",
        "#  --input_arrays='normalized_input_image_tensor' \\\n",
        "#  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "#  --mean_values=128 \\\n",
        "#  --std_dev_values=128 \\\n",
        "#  --input_shapes=1,300,300,3 \\\n",
        "#  --change_concat_input_ranges=false \\\n",
        "#  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "#  --allow_custom_ops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0yu_O4AtotZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!echo \"CONVERTING frozen graph to unquantized TF Lite file...\"\n",
        "#!tflite_convert \\\n",
        "#  --output_file='{output_directory}/road_signs_float.tflite' \\\n",
        "#  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "# --input_arrays='normalized_input_image_tensor' \\\n",
        "#  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "#  --mean_values=128 \\\n",
        "#  --std_dev_values=128 \\\n",
        "#  --input_shapes=1,300,300,3 \\\n",
        "#  --change_concat_input_ranges=false \\\n",
        "#  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "# --allow_custom_ops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_yUkAYgtota",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(output_directory)\n",
        "!ls -ltra '{output_directory}'\n",
        "#pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\") # this is main one\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
        "!cp '{label_map_pbtxt_fname}' '{output_directory}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKCQv8ihHtmD",
        "colab_type": "text"
      },
      "source": [
        "##Run Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG32lnhItotb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "print(PATH_TO_CKPT)\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "#PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images/test\")\n",
        "PATH_TO_TEST_IMAGES_DIR='/content/drive/My Drive/Colab Notebooks/test_data'\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.png\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQSSwf2stotc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh9mIKqfdQtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_output_for_single_image(output_dict):\n",
        "  score_threshold=0.5\n",
        "  labels=[]\n",
        "  for i in range(len(output_dict['detection_scores'])):\n",
        "    if output_dict['detection_scores'][i]>score_threshold:\n",
        "      if (output_dict['detection_boxes'][i][2]-output_dict['detection_boxes'][i][0])>0.05:\n",
        "        labels.append(output_dict['detection_classes'][i])\n",
        "    else:\n",
        "      break\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EqsKp1Ftotd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## running inferences.  This should show images with bounding boxes\n",
        "#%matplotlib inline\n",
        "#\n",
        "#print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
        "#for image_path in TEST_IMAGE_PATHS:\n",
        "#    image = Image.open(image_path)\n",
        "#    # the array based representation of the image will be used later in order to prepare the\n",
        "#    # result image with boxes and labels on it.\n",
        "#    image_np = load_image_into_numpy_array(image)\n",
        "#    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "#    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "#    # Actual detection.\n",
        "#    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "#    # Visualization of the results of a detection.\n",
        "#    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "#        image_np,\n",
        "#        output_dict['detection_boxes'],\n",
        "#        output_dict['detection_classes'],\n",
        "#        output_dict['detection_scores'],\n",
        "#        category_index,\n",
        "#        instance_masks=output_dict.get('detection_masks'),\n",
        "#        use_normalized_coordinates=True,\n",
        "#        line_thickness=2)\n",
        "#    plt.figure(figsize=IMAGE_SIZE)\n",
        "#    plt.imshow(image_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VXf2YhoR_x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Too many figures would cause RuntimeError, so just plot a batch of figures.\n",
        "# running inferences.  This should show images with bounding boxes\n",
        "import re\n",
        "%matplotlib inline\n",
        "\n",
        "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
        "label_lists=[[] for i in range(len(TEST_IMAGE_PATHS))]\n",
        "boxes_bottom_mid=[[] for i in range(len(TEST_IMAGE_PATHS))]\n",
        "boxes_bottom=[[] for i in range(len(TEST_IMAGE_PATHS))]\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "#image_path=TEST_IMAGE_PATHS[31]\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=2)\n",
        "    #plt.figure(figsize=IMAGE_SIZE)\n",
        "    #plt.imshow(image_np)\n",
        "    image_index=int(re.findall(r'(\\d+).png',image_path)[0])\n",
        "    print(image_index)\n",
        "    label_list=label_output_for_single_image(output_dict)\n",
        "    label_lists[image_index-1]=label_list\n",
        "    for i in range(len(output_dict['detection_classes'])):\n",
        "      if output_dict['detection_scores'][i]>0.5 and output_dict['detection_classes'][i] in [1,10]:\n",
        "        boxes_bottom_mid[image_index-1].append((output_dict['detection_boxes'][i][1]+output_dict['detection_boxes'][i][3])/2)\n",
        "        boxes_bottom[image_index-1].append(output_dict['detection_boxes'][i][2])\n",
        "\n",
        "print(label_lists)\n",
        "\n",
        "print(boxes_bottom_mid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU_BexU4m6D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "def location_detection(i,bottom_mid,bottom):\n",
        "  path=os.path.join(PATH_TO_TEST_IMAGES_DIR,str(i+1)+'.png')\n",
        "  img=cv2.imread(path)\n",
        "  #plt.imshow(img)\n",
        "  height, width, _ = img.shape\n",
        "  img = img[int(height/2):,:,:]\n",
        "  gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.inRange(gray,0,10)\n",
        "  ret, binary = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n",
        "  kernel = np.ones((5, 5), np.uint8)\n",
        "  dilation = cv2.dilate(binary, kernel, iterations=1)\n",
        "  #plt.imshow(dilation)\n",
        "  lines = cv2.HoughLinesP(\n",
        "    binary,\n",
        "    rho=1,\n",
        "    theta=np.pi / 180,\n",
        "    threshold=10,\n",
        "    lines=np.array([]),\n",
        "    minLineLength=8,\n",
        "    maxLineGap=4\n",
        ")\n",
        "  \n",
        "  #print(lines)\n",
        "  for line in lines:\n",
        "      for x1, y1, x2, y2 in line:\n",
        "          cv2.line(img,(x1,y1),(x2,y2),((255, 0, 0)),2)\n",
        "  #plt.imshow(img)\n",
        "  \n",
        "  def make_points(frame, line):\n",
        "    height, width, _ = frame.shape\n",
        "    slope, intercept = line\n",
        "    intercept=intercept+height\n",
        "    y1 = height*2  # bottom of the frame\n",
        "    #y2 = int(y1 * 1 / 2)  # make points from middle of the frame down\n",
        "    y2 = 0\n",
        "    # bound the coordinates within the frame\n",
        "    x1 = max(-width, min(2 * width, int((y1 - intercept) / slope)))\n",
        "    x2 = max(-width, min(2 * width, int((y2 - intercept) / slope)))\n",
        "    return [[x1, y1, x2, y2]]\n",
        "\n",
        "  lane_lines = []\n",
        "  height, width, _ = img.shape\n",
        "  left_fit = []\n",
        "  right_fit = []\n",
        "  boundary = 1/3\n",
        "  left_region_boundary = width * (1 - boundary)  # left lane line segment should be on left 2/3 of the screen\n",
        "  right_region_boundary = width * boundary # right lane line segment should be on left 2/3 of the screen\n",
        "  for line_segment in lines:\n",
        "      for x1, y1, x2, y2 in line_segment:\n",
        "        try:\n",
        "          fit = np.polyfit((x1, x2), (y1, y2), 1)\n",
        "          \n",
        "          slope = fit[0]\n",
        "          intercept = fit[1]\n",
        "          if slope < 0:\n",
        "              if x1 < left_region_boundary and x2 < left_region_boundary:\n",
        "                  left_fit.append((slope, intercept))\n",
        "          else:\n",
        "              if x1 > right_region_boundary and x2 > right_region_boundary:\n",
        "                  right_fit.append((slope, intercept))\n",
        "        except:\n",
        "          pass\n",
        "  \n",
        "  if len(left_fit) > 0:\n",
        "      left_fit_average = np.average(left_fit, axis=0)\n",
        "      lane_lines.append(make_points(img, left_fit_average))\n",
        "  else:\n",
        "      left_fit_average=[]\n",
        "  \n",
        "  if len(right_fit) > 0:\n",
        "      right_fit_average = np.average(right_fit, axis=0)\n",
        "      lane_lines.append(make_points(img, right_fit_average))\n",
        "  else:\n",
        "      right_fit_average=[]\n",
        "  #print(lane_lines)\n",
        "  image=cv2.imread(path)\n",
        "  for line in lane_lines:\n",
        "      for x1, y1, x2, y2 in line:\n",
        "          cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "  plt.imshow(image)\n",
        "  for i in range(len(bottom)):\n",
        "    y=bottom[i]*height\n",
        "    x=bottom_mid[i]*width\n",
        "    if len(left_fit_average)!=0 and len(right_fit_average)==0:\n",
        "      if y>left_fit_average[0]*x+left_fit_average[1]+height/2:\n",
        "        return 0\n",
        "    elif len(left_fit_average)==0 and len(right_fit_average)!=0:\n",
        "      if y>right_fit_average[0]*x+right_fit_average[1]+height/2:\n",
        "        return 0\n",
        "    elif len(left_fit_average)!=0 and len(right_fit_average)!=0:\n",
        "      if x<(y-right_fit_average[1]-height/2)/right_fit_average[0] and x>(y-left_fit_average[1]-height/2)/left_fit_average[0]:\n",
        "        return 0\n",
        "  return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_WKpPBXUX-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_sign=[3,4,6,8,9]\n",
        "judge_sign=[1,10]\n",
        "norm_speed=np.ones(len(label_lists),dtype=np.int32)\n",
        "for i in range(len(label_lists)):\n",
        "  for j in range(len(label_lists[i])):\n",
        "    if label_lists[i][j] in stop_sign:\n",
        "      norm_speed[i]=0\n",
        "      break\n",
        "    if label_lists[i][j] in judge_sign:\n",
        "      print(i)\n",
        "      norm_speed[i]=location_detection(i,boxes_bottom_mid[i],boxes_bottom[i])\n",
        "      if norm_speed[i]==0:\n",
        "        break\n",
        "print(norm_speed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7kiXHNjPsFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "table=pd.DataFrame(norm_speed,index=list(range(1,len(label_lists)+1)),columns=['speed'])\n",
        "table.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GbeamA_QT9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table['image_id']=list(range(1,len(label_lists)+1))\n",
        "table=table.reindex(columns=['image_id','speed'])\n",
        "table.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ko7rdMzrmmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table.to_csv('/content/drive/My Drive/Colab Notebooks/speed.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBEVGkpDE7am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}